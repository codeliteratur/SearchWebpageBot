{"cells":[{"metadata":{},"cell_type":"markdown","source":"# SearchPageBot replies queries from contents in a webpage."},{"metadata":{},"cell_type":"markdown","source":" This is another example of a chatbot that takes data from a webpage and the chatbot replies to queries related to contents of the page. You need to input the URL of your desired webpage. \n \n# Import necessary libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nimport numpy as np\nimport random\nimport string","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Natural Language Toolkit (NLTK) is an open source Python library for Natural Language Processing.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"from bs4 import BeautifulSoup","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Beautiful Soup uses a pluggable XML or HTML parser to parse a (possibly invalid) document into a tree representation. Beautiful Soup provides methods and Pythonic idioms that make it easy to navigate, search, and modify the parse tree. \nThere are mainly two ways to extract data from a website:\n\n- Use the API of the website (if it exists). For example, Facebook has the Facebook Graph API which allows retrieval of data posted on Facebook.\n- Access the HTML of the webpage and extract useful information/data from it. This technique is called web scraping or web harvesting or web data extraction."},{"metadata":{},"cell_type":"markdown","source":"Easiest way to install external libraries in python is to use pip. pip is a package management system used to install and manage software packages written in Python.\nAll you need to do is:"},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install requests\npip install html5lib\npip install bs4","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Accessing the HTML content from webpage"},{"metadata":{"trusted":true},"cell_type":"code","source":"import requests \nURL = \"https://www.wikipedia.org/data-structures/\"\nr = requests.get(URL) \nprint(r.content) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"soup = BeautifulSoup(r.content, 'html5lib') # If this line causes an error, run 'pip install html5lib' or install html5lib \nprint(soup.prettify()) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Searching and navigating through the parse tree"},{"metadata":{},"cell_type":"markdown","source":"Now, we would like to extract some useful data from the HTML content. The soup object contains all the data in the nested structure which could be programmatically extracted. In our example, we are scraping a webpage consisting of data about something. So, we would like to create a program to save those sentences (and all relevant information about them)."},{"metadata":{"trusted":true},"cell_type":"code","source":"import bs4 as bs\nimport urllib.request\nimport re\nprint(\"Hello, I am GoogleSearch. Give me the URL of a webpage you want answers from here ->  \")\nraw_html = urllib.request.urlopen(input())\nraw_html = raw_html.read()\n\narticle_html = bs.BeautifulSoup(raw_html, 'lxml')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The urllib.request module defines functions and classes which help in opening URLs (mostly HTTP) in a complex world â€” basic and digest authentication, redirections, cookies and more. \nThe urllib.request module defines the following functions:\n\nurllib.request.urlopen(url, data=None, [timeout, ]*, cafile=None, capath=None, cadefault=False, context=None)\nOpen the URL url, which can be either a string or a Request object."},{"metadata":{"trusted":true},"cell_type":"code","source":"article_paragraphs = article_html.find_all('p')\n\narticle_text = ''\n\nfor para in article_paragraphs:\n    article_text += para.text\n\narticle_text = article_text.lower()\narticle_text = re.sub(r'\\[[0-9]*\\]', ' ', article_text)\narticle_text = re.sub(r'\\s+', ' ', article_text)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tokenisation"},{"metadata":{"trusted":true},"cell_type":"code","source":"article_sentences = nltk.sent_tokenize(article_text)\narticle_words = nltk.word_tokenize(article_text)\nwnlemmatizer = nltk.stem.WordNetLemmatizer()\ndef perform_lemmatization(tokens):\n    return [wnlemmatizer.lemmatize(token) for token in tokens]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Converting the context to sentences and words. We shall now define a function called perform_lemmatization which will take as input the tokens and return normalized tokens."},{"metadata":{"trusted":true},"cell_type":"code","source":"punctuation_removal = dict((ord(punctuation), None) for punctuation in string.punctuation)\n\ndef get_processed_text(document):\n    return perform_lemmatization(nltk.word_tokenize(document.lower().translate(punctuation_removal)))\ngreeting_inputs = (\"hey\", \"good morning\", \"good evening\", \"morning\", \"evening\", \"hi\", \"whatsup\")\ngreeting_responses = [\"hey\", \"hey hows you?\", \"*nods*\", \"hello, how you doing\", \"hello\", \"Welcome, I am good and you\"]\ndef generate_greeting_response(greeting):\n    for token in greeting.split():\n        if token.lower() in greeting_inputs:\n            return random.choice(greeting_responses)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Machine learning library to extract words according to stop word"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Matching the keywords in user's responses."},{"metadata":{"trusted":true},"cell_type":"code","source":"word_vectorizer = TfidfVectorizer(tokenizer=get_processed_text, stop_words='english')\n    all_word_vectors = word_vectorizer.fit_transform(article_sentences)\n    similar_vector_values = cosine_similarity(all_word_vectors[-1], all_word_vectors)\n    similar_sentence_number = similar_vector_values.argsort()[0][-2]\n\n    matched_vector = similar_vector_values.flatten()\n    matched_vector.sort()\n    vector_matched = matched_vector[-2]\n\n    if vector_matched == 0:\n        bot_response = bot_response + \"I am sorry, I could not understand you!\"\n        return bot_response\n    else:\n        bot_response = bot_response + article_sentences[similar_sentence_number]\n        return bot_response\n\nword_vectorizer = TfidfVectorizer(tokenizer=get_processed_text, stop_words='english')\nall_word_vectors = word_vectorizer.fit_transform(article_sentences)\nsimilar_vector_values = cosine_similarity(all_word_vectors[-1], all_word_vectors)\nsimilar_sentence_number = similar_vector_values.argsort()[0][-2]\ncontinue_dialogue = True","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building the conversation "},{"metadata":{"trusted":true},"cell_type":"code","source":"while(continue_dialogue == True):\n    print(\" Ask your question:  \")\n    user_text = input()\n    user_text = user_text.lower()\n    if user_text != 'bye':\n        if user_text == 'thanks' or user_text == 'thank you very much' or user_text == 'thank you':\n            continue_dialogue = False\n            print(\"Google Search: Most welcome\")\n        else:\n            if generate_greeting_response(user_text) != None:\n                print(\"Google Search: \" + generate_greeting_response(user_text))\n            else:\n                print(\"Google Search: \", end=\"\")\n                print(generate_response(user_text))\n                article_sentences.remove(user_text)\n    else:\n        continue_dialogue = False\n        print(\"Google Search: Good bye and take care. Come back again if you want to know something...\")\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}